<html>
<head>
<title>How to start Penetration testing of Artificial Intelligence</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何开始人工智能的渗透测试</h1>
<blockquote>原文：<a href="https://infosecwriteups.com/how-to-start-penetration-testing-of-artificial-intelligence-c11e97b77dfa?source=collection_archive---------0-----------------------#2022-07-02">https://infosecwriteups.com/how-to-start-penetration-testing-of-artificial-intelligence-c11e97b77dfa?source=collection_archive---------0-----------------------#2022-07-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="56a5" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">Pentesting需要发展以发现基于人工智能的风险</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/08f148519a33ad407cd4e46c60d1705d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*fwlCfUHYfuEB_DaK.png"/></div></div></figure><p id="a482" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在过去的几十年里，软件安全已经走过了漫长的道路。现在很难相信，但是曾经有一段时间<strong class="kw iu">渗透测试</strong>只在主机/网络层进行，安全团队完全知道应用级攻击，如<strong class="kw iu"> SQL注入、跨站点脚本</strong>等。因此，攻击者开始通过攻击应用层来绕过网络防火墙等传统防御，我们看到了Web应用防火墙(WAF)、Appsec审查、安全代码等控制措施的出现。</p><p id="6dff" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">今天，软件安全已经发展到通过DevSecOps运动涵盖无服务器、API控制和管道安全等技术</p><blockquote class="lq"><p id="8cc9" class="lr ls it bd lt lu lv lw lx ly lz lp dk translated"><strong class="ak"> <em class="ma">这历史课有什么意义？概括来说，今天的软件是建立在以前的技术经验之上的，我们面临着在安全审查和渗透测试</em> </strong>中没有涉及的基于人工智能的系统上重复同样错误的危险</p></blockquote><h1 id="7cba" class="mb mc it bd md me mf mg mh mi mj mk ml jz mm ka mn kc mo kd mp kf mq kg mr ms bi translated">新出现的威胁</h1><p id="5b32" class="pw-post-body-paragraph ku kv it kw b kx mt ju kz la mu jx lc ld mv lf lg lh mw lj lk ll mx ln lo lp im bi translated">正如安全团队所知，黑客是出了名的聪明。一旦在应用程序中实施了一层安全措施，他们就会将目标转移到不具备相同成熟度的新兴技术上。请记住，今天基于人工智能的系统是过去的web应用程序，即网络安全团队没有意识到的一个全新的攻击领域。</p><p id="82e1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">对于实施基于人工智能的系统的公司，网络安全团队通常会对软件栈进行渗透测试，直到应用层，但<em class="my">忽略了一个关键点</em>:</p><blockquote class="mz na nb"><p id="0867" class="ku kv my kw b kx ky ju kz la lb jx lc nc le lf lg nd li lj lk ne lm ln lo lp im bi translated">基于人工智能(AI)的系统最重要的特征是根据提供的数据做出决策的能力。<em class="it">如果这些数据和决策能力遭到破坏或窃取，那么整个人工智能生态系统都会受到损害</em></p></blockquote><p id="152b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">不幸的是，今天的大多数网络安全团队都没有意识到人工智能系统引入的新型攻击。在这些攻击中，攻击者操纵人工智能系统如何工作的独特特征，以利于他的恶意意图。</p><p id="3d3f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">许多商业模型已经被操纵或欺骗，这种类型的攻击只会随着人工智能的大规模采用而增加。</p><h1 id="0d68" class="mb mc it bd md me mf mg mh mi mj mk ml jz nf ka mn kc ng kd mp kf nh kg mr ms bi translated">人工智能攻击的独特类型</h1><p id="843f" class="pw-post-body-paragraph ku kv it kw b kx mt ju kz la mu jx lc ld mv lf lg lh mw lj lk ll mx ln lo lp im bi translated">人工智能引入的新威胁面相当多样化。数据可能会被<strong class="kw iu">有意或无意地</strong>毒害，从而导致被操纵的决策。类似地，一旦攻击者弄清楚了底层的决策逻辑，人工智能逻辑或数据可以被"<strong class="kw iu"/>"推断出来，导致数据提取，而模型可以被"<strong class="kw iu"/>"回避。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/928dc4766b6b493c373a41712de3cc56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JxqOnz16R-xI43iR01TT_g.png"/></div></div><figcaption class="nj nk gj gh gi nl nm bd b be z dk translated">来源:作者</figcaption></figure><p id="175f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如果这听起来有点太模糊，那么下面将深入介绍这些攻击的更多细节:</p><p id="f502" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">⚠ ️ <strong class="kw iu">数据中毒</strong>:攻击者可以毒害用于训练机器学习模型的训练数据。通过污染这个数据源，攻击者可以创建一个“后门”,因为他知道该模型是根据错误数据训练的，并且知道如何利用它。这有助于进一步的攻击，如后面提到的模型规避。</p><p id="ca11" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">⚠ ️ <strong class="kw iu">模型中毒</strong>:和之前的攻击一样，但是这次攻击者的目标是模型而不是数据。一个预先训练好的模型被破坏并注入了后门，攻击者可以利用这些后门绕过它的决策过程。大多数公司不是从零开始建立模型，而是使用预先训练的模型，这些模型通常是可用的，如微软的<a class="ae nn" href="https://www.microsoft.com/en-us/research/blog/microsoft-vision-model-resnet-50-combines-web-scale-data-and-multi-task-learning-to-achieve-state-of-the-art/" rel="noopener ugc nofollow" target="_blank"> ResNet </a>或<a class="ae nn" href="https://openai.com/blog/clip/" rel="noopener ugc nofollow" target="_blank"> Clip </a> OpenAI。这些模型存储在模型动物园中，这是开源框架和公司组织其机器学习和深度学习模型的常见方式。这就像一个软件供应链攻击，攻击者可以对许多用户下毒</p><p id="db5c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">⚠ ️ <strong class="kw iu">数据提取</strong>:攻击者可以查询模型，了解在学习中使用了哪些训练数据。这可能导致敏感数据的泄露，因为攻击者可以推断出模型训练中使用的数据，如果涉及敏感数据，这将特别危险。这种类型的攻击也称为“成员推理”,不需要访问模型的功能，只需观察模型的输出就可以完成</p><p id="3ddb" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">⚠ <strong class="kw iu"> ️Model提取:</strong>攻击者可以通过反复查询模型并观察其功能来创建模型的离线副本。事实上，大多数模型公开地暴露了它们的API，并且没有适当地净化它们的输出，这可能会助长这些攻击。这种技术允许攻击者深入分析离线拷贝，并了解如何绕过生产模型</p><p id="262c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">⚠ <strong class="kw iu"> ️Model规避:攻击者</strong>通过提供特定的输入来欺骗模型，从而导致错误的决策。这通常通过观察运行中的模型并理解如何绕过它来实现。例如，攻击者可以试图欺骗基于人工智能的反恶意软件系统不检测他们的样本或绕过生物特征验证系统。</p><p id="2fdd" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">毫无疑问，随着人工智能应用的增加；上述攻击将变得像今天的SQL注入一样普遍，并且将被一并提及。防范这些攻击的最佳解决方案之一是通过将它们合并到当前的渗透测试实践中来进行早期检测。</p><h1 id="36c8" class="mb mc it bd md me mf mg mh mi mj mk ml jz nf ka mn kc ng kd mp kf nh kg mr ms bi translated">笔测试人工智能应用</h1><p id="a8a8" class="pw-post-body-paragraph ku kv it kw b kx mt ju kz la mu jx lc ld mv lf lg lh mw lj lk ll mx ln lo lp im bi translated">好消息是你不必从头开始。如果你曾经作为网络安全团队的一员参与过渗透测试或red teaming，那么你可能会熟悉<a class="ae nn" href="https://attack.mitre.org/" rel="noopener ugc nofollow" target="_blank"> <strong class="kw iu">米特ATT &amp; CK </strong> </a> <strong class="kw iu"> </strong>框架，这是一个基于真实世界示例的可公开访问的对手攻击和技术框架。</p><p id="6247" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">它在全球范围内被各种公共和私营部门组织用于威胁模型和风险评估。任何人都可以访问它，并了解攻击者将用来攻击特定系统的战术和技术，这对参与渗透测试或红队的人非常有用。</p><p id="6fed" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这个流行的框架被用作创建<a class="ae nn" href="https://atlas.mitre.org/" rel="noopener ugc nofollow" target="_blank"> <strong class="kw iu"> MITRE ATLAS </strong> </a>(人工智能系统的对抗性威胁格局)的模型，它被描述为</p><p id="0ee0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><em class="my">“机器学习(ML)系统的对手战术、技术和案例研究的知识库，基于真实世界的观察、ML red团队和安全小组的演示以及学术研究的可能状态</em>”</p><p id="2f6c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">ATLAS遵循与MITRE相同的框架，因此当网络安全从业者想要测试其内部人工智能系统的漏洞和安全风险时，他们可以非常容易地研究和采用它的技术。它还有助于在网络安全社区中建立对这些风险的意识，因为这些风险是以他们已经熟悉的格式呈现的。</p><h1 id="7e82" class="mb mc it bd md me mf mg mh mi mj mk ml jz nf ka mn kc ng kd mp kf nh kg mr ms bi translated">人工智能渗透测试工具</h1><p id="64b6" class="pw-post-body-paragraph ku kv it kw b kx mt ju kz la mu jx lc ld mv lf lg lh mw lj lk ll mx ln lo lp im bi translated">标准安全工具通常没有内置基于人工智能的技术，这些技术可以评估模型对模型推断或规避等风险的脆弱性。值得庆幸的是，网络安全团队可以使用免费工具来补充他们现有的渗透测试工具包。这些工具是开源的，但是你也可以寻找商业替代品。</p><p id="739a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">无论您喜欢哪种类型，都要确保工具具有以下特征。</p><ol class=""><li id="52b0" class="no np it kw b kx ky la lb ld nq lh nr ll ns lp nt nu nv nw bi translated"><strong class="kw iu">模型不可知:</strong>可以测试所有类型的模型，不局限于任何一个特定的模型</li><li id="fb2c" class="no np it kw b kx nx la ny ld nz lh oa ll ob lp nt nu nv nw bi translated"><strong class="kw iu">技术不可知</strong>:它应该能够测试托管在任何平台上的人工智能模型，无论是在云上还是在本地。</li><li id="ff4a" class="no np it kw b kx nx la ny ld nz lh oa ll ob lp nt nu nv nw bi translated"><strong class="kw iu">与您现有的工具包集成:</strong>应该具有命令行功能，以便为您的安全团队轻松编写脚本和实现自动化。</li></ol><p id="ac14" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">您可以找到的一些免费工具有</p><ul class=""><li id="5b70" class="no np it kw b kx ky la lb ld nq lh nr ll ns lp oc nu nv nw bi translated"><a class="ae nn" href="https://github.com/Azure/counterfit" rel="noopener ugc nofollow" target="_blank"> <strong class="kw iu">由微软</strong> </a>抗衡:被微软描述为“作为开源项目的一个<em class="my"> n用于安全测试AI系统的自动化工具。Counterfit帮助组织进行人工智能安全风险评估，以确保其业务中使用的算法是健壮、可靠和值得信赖的</em>。Counterfit提供了一种很好的方式来自动化和测试针对AI系统的攻击，并且可以在红队和渗透测试中使用。它包含预加载的人工智能攻击模式，安全专业人员可以通过脚本从命令行运行，并可以与现有的工具包集成</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi od"><img src="../Images/e16b3cfb8a3fae0620635578c7ee55cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*y8B1X4y3X-VIrXis"/></div></div></figure><ul class=""><li id="1fc5" class="no np it kw b kx ky la lb ld nq lh nr ll ns lp oc nu nv nw bi translated"><a class="ae nn" href="https://github.com/Trusted-AI/adversarial-robustness-toolbox" rel="noopener ugc nofollow" target="_blank"> <strong class="kw iu">对抗性鲁棒性工具箱(ART) </strong> </a>被描述为<em class="my">一个用于机器学习安全的Python库。ART提供工具，使开发人员和研究人员能够防御和评估机器学习模型和应用程序，抵御规避、中毒、提取和推理的对抗性威胁。ART支持所有流行的机器学习框架"</em></li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ca"><img src="../Images/b6902b5a05f97dc75346229c000cc749.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vHCf6u4yjNLmtVDlprXBcg.png"/></div></div><figcaption class="nj nk gj gh gi nl nm bd b be z dk translated">来源:作者</figcaption></figure><p id="b3fa" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为了使这些工具有效，请确保将其映射到<strong class="kw iu"> ATLAS </strong>框架，以便您可以将其与通用标准保持一致。您可以使用这些工具进行red团队/渗透测试以及进行AI系统的漏洞评估。使用它们来定期扫描您的人工智能资产，并建立人工智能特定风险的风险跟踪器。通过随着时间的推移跟踪这些风险，您可以看到您的安全状况有所改善，并随着时间的推移监控进度。</p><p id="f300" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">获得更好的攻击背景和意识的另一个有价值的资源是此处<a class="ae nn" href="https://atlas.mitre.org/studies" rel="noopener ugc nofollow" target="_blank">列出的Atlas案例研究页面</a>。这个页面列出了对生产AI系统的已知攻击，安全团队可以使用它来更好地了解对他们系统的影响。</p><p id="f725" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我希望这是有用的，给你一个将AI渗透测试添加到你的网络安全保证活动的起点。请放心，由于人工智能应用的大量增加以及网络罪犯对滥用人工智能的兴趣，这个领域将在未来几年内大受欢迎。</p><blockquote class="lq"><p id="bf9e" class="lr ls it bd lt lu lv lw lx ly lz lp dk translated"><strong class="ak">祝您的人工智能安全之旅好运！如果你有兴趣了解更多，那么可以查看我的关于AI安全的书</strong> <a class="ae nn" href="https://www.amazon.com/Artificial-Intelligence-Governance-Cyber-Security-beginners/dp/B09YHK8L2T/" rel="noopener ugc nofollow" target="_blank"> <strong class="ak">这里</strong> </a> <strong class="ak">或者我的打折大师班关于题目</strong> <a class="ae nn" href="https://cloudsecguy.gumroad.com/l/aigovernance/1tojq7p?_gl=1*1c51k6t*_ga*MzQ0NDEyMjc4LjE2NDM3MTgwOTU.*_ga_6LJN6D94N6*MTY2MzA5NTE4Ni4yNzEuMS4xNjYzMDk1MTkzLjAuMC4w" rel="noopener ugc nofollow" target="_blank"> <strong class="ak">这里</strong> </a></p></blockquote><div class="oe of og oh oi oj"><a href="https://taimurcloud123.medium.com/membership" rel="noopener follow" target="_blank"><div class="ok ab fo"><div class="ol ab om cl cj on"><h2 class="bd iu gy z fp oo fr fs op fu fw is bi translated">通过我的推荐链接加入Medium-Taimur Ijlal</h2><div class="oq l"><h3 class="bd b gy z fp oo fr fs op fu fw dk translated">阅读Taimur Ijlal(以及媒体上成千上万的其他作家)的每一个故事。您的会员费直接支持…</h3></div><div class="or l"><p class="bd b dl z fp oo fr fs op fu fw dk translated">taimurcloud123.medium.com</p></div></div><div class="os l"><div class="ot l ou ov ow os ox ks oj"/></div></div></a></div></div><div class="ab cl oy oz hx pa" role="separator"><span class="pb bw bk pc pd pe"/><span class="pb bw bk pc pd pe"/><span class="pb bw bk pc pd"/></div><div class="im in io ip iq"><h2 id="162a" class="pf mc it bd md pg ph dn mh pi pj dp ml ld pk pl mn lh pm pn mp ll po pp mr pq bi translated">来自Infosec的报道:Infosec每天都有很多内容，很难跟上。<a class="ae nn" href="https://weekly.infosecwriteups.com/" rel="noopener ugc nofollow" target="_blank">加入我们的每周简讯</a>以5篇文章、4个线程、3个视频、2个Github Repos和工具以及1个工作提醒的形式免费获取所有最新的Infosec趋势！</h2></div></div>    
</body>
</html>